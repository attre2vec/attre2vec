{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import yaml\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_metrics(path, baseline=True):\n",
    "    with open(path, 'rb') as fin:\n",
    "        return pickle.load(fin)\n",
    "\n",
    "\n",
    "def load_all_metrics(models):\n",
    "    all_metrics = {ds: {} for ds in models['datasets']}\n",
    "    \n",
    "    # Baselines\n",
    "    for ds in models['datasets']:\n",
    "        all_metrics[ds].update({\n",
    "            f'baseline/{variant}/{bl_cfg[\"name\"]}': load_metrics(\n",
    "                models['bl']['base_path']\n",
    "                .replace('${DATASET}', ds)\n",
    "                .replace('${NAME}', bl_cfg['name'])\n",
    "                .replace('${VARIANT}', variant)\n",
    "            )\n",
    "            for bl_cfg in models['bl']['models']\n",
    "            for variant in bl_cfg['variants']\n",
    "        })\n",
    "        \n",
    "        all_metrics[ds].update({\n",
    "            'baseline/line2vec': load_metrics(\n",
    "                models['line2vec']['base_path']\n",
    "                .replace('${DATASET}', ds)\n",
    "            )\n",
    "        })\n",
    "        \n",
    "    # MLP baselines\n",
    "    for ds in models['datasets']:\n",
    "        all_metrics[ds].update({\n",
    "            f'{mlptype}/{name}': load_metrics(\n",
    "                models[mlptype]['base_path']\n",
    "                .replace('${DATASET}', ds)\n",
    "                .replace('${NAME}', name)\n",
    "            )\n",
    "            for mlptype in ('mlp2', 'mlp3')\n",
    "            for name in models[mlptype]['models']\n",
    "        })\n",
    "    \n",
    "    # AttrE2vec models\n",
    "    for ds in models['datasets']:\n",
    "        all_metrics[ds].update({\n",
    "            f'ae/{ae_name}': load_metrics(\n",
    "                models['ae']['base_path']\n",
    "                .replace('${DATASET}', ds)\n",
    "                .replace('${NAME}', ae_name)\n",
    "            )\n",
    "            for ae_name in models['ae']['models']\n",
    "        })\n",
    "    \n",
    "    return all_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_supports(all_metrics):\n",
    "    display(Markdown('# Supports'))\n",
    "    \n",
    "    for ds_name in all_metrics.keys():\n",
    "        supports = []\n",
    "        method_name = list(all_metrics[ds_name].keys())[0]\n",
    "        for dss_metrics in all_metrics[ds_name][method_name]:\n",
    "            for tt, cm in dss_metrics.items():\n",
    "                for c, v in cm.items():\n",
    "                    if c in ('accuracy', 'auc', 'cm') or 'avg' in c:\n",
    "                        continue\n",
    "                    supports.append((tt, c, v['support']))   \n",
    "    \n",
    "        df = pd.DataFrame.from_records(supports, columns=['train/val/test', 'class', 'support'])\n",
    "        df = df.groupby(['train/val/test', 'class']).agg(['mean', 'std'])\n",
    "        df['summary'] = df['support'].apply(lambda r: f'{int(r[\"mean\"])} +/- {int(r[\"std\"])}', axis=1)\n",
    "        df = df.drop(columns='support').reset_index()\n",
    "\n",
    "        df = df.pivot(columns='class', values='summary', index='train/val/test')\n",
    "        df = df.reindex(index=['train', 'val', 'test'])\n",
    "                                            \n",
    "        display(Markdown(f'## {ds_name}'))\n",
    "        display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_max(s):\n",
    "    '''\n",
    "    highlight the maximum in a Series yellow.\n",
    "    '''\n",
    "    is_max = s == s.max()\n",
    "    return ['background-color: yellow' if v else '' for v in is_max]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_auc_table(all_metrics):\n",
    "    display(Markdown('# AUC'))\n",
    "    for ds_name, ds_mtrs in all_metrics.items():\n",
    "        display(Markdown(f'## {ds_name}'))\n",
    "        vals = []\n",
    "\n",
    "        for model_name, model_mtrs in ds_mtrs.items():\n",
    "            for tt in ('train', 'val', 'test'):\n",
    "                aucs = []\n",
    "                for sample_mtrs in model_mtrs:\n",
    "                    aucs.append(sample_mtrs[tt]['auc'])\n",
    "\n",
    "                mean = np.round(np.mean(aucs) * 100.0, 2)\n",
    "                std = np.round(np.std(aucs) * 100.0, 2)\n",
    "\n",
    "                vals.append((model_name, tt, f'{mean} +/- {std}'))\n",
    "\n",
    "        df = pd.DataFrame.from_records(vals, columns=['model', 'tt', 'value'])\n",
    "        df = df.pivot(index='model', columns='tt')\n",
    "        df = df.reindex(columns=['train', 'val', 'test'], level='tt')\n",
    "\n",
    "        for group in ('baseline', 'ae'):\n",
    "            _df = df.loc[df.index.str.startswith(group)]\n",
    "            _df = _df.style.apply(highlight_max)\n",
    "            display(_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def print_metric_table(all_metrics, metric_name):\n",
    "    display(Markdown(f'# {metric_name}'))\n",
    "    \n",
    "    for ds_name, ds_mtrs in all_metrics.items():\n",
    "        display(Markdown(f'# {ds_name}'))\n",
    "        vals = []\n",
    "\n",
    "        for model_name, model_mtrs in ds_mtrs.items():\n",
    "            for tt in ('train', 'val', 'test'):\n",
    "                mtrs = defaultdict(list)\n",
    "                for sample_mtrs in model_mtrs:\n",
    "                    for c in sample_mtrs[tt]:\n",
    "                        if c in ('accuracy', 'auc', 'cm'):\n",
    "                            continue\n",
    "\n",
    "                        mtrs[c].append(sample_mtrs[tt][c][metric_name])\n",
    "\n",
    "                for c, mtrs_vals in mtrs.items():\n",
    "                    mean = np.round(np.mean(mtrs_vals) * 100.0, 2)\n",
    "                    std = np.round(np.std(mtrs_vals) * 100.0, 2)\n",
    "\n",
    "                    vals.append((model_name, tt, c, f'{mean} +/- {std}'))\n",
    "\n",
    "        df = pd.DataFrame.from_records(vals, columns=['model', 'tt', 'class', 'value'])\n",
    "        df = pd.pivot_table(df, index='model', columns=['tt', 'class'], values='value', aggfunc='first')\n",
    "        df = df.reindex(columns=['train', 'val', 'test'], level='tt')\n",
    "\n",
    "        for group in ('baseline', 'ae'):\n",
    "            _df = df.loc[df.index.str.startswith(group)]\n",
    "            _df = _df.style.apply(highlight_max)\n",
    "            display(_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_article_table(all_metrics):\n",
    "    def _agg_test_metric(sm, name):\n",
    "        vals = [\n",
    "            m['test']['macro avg']['f1-score'] if name == 'f1' else m['test'][name]\n",
    "            for m in sm\n",
    "        ]\n",
    "        mean = np.round(np.mean(vals) * 100.0, 2)\n",
    "        std = np.round(np.std(vals) * 100.0, 2)\n",
    "        \n",
    "        return f'{mean} +/- {std}'\n",
    "    \n",
    "    records = []\n",
    "    for ds_name, ds_mtrs in all_metrics.items():\n",
    "        for model_name, model_mtrs in ds_mtrs.items():\n",
    "            for metric_name in ('auc',):#('auc', 'accuracy', 'f1'):\n",
    "                metric_val = _agg_test_metric(model_mtrs, metric_name)\n",
    "                records.append((ds_name, model_name, metric_name, metric_val))\n",
    "                \n",
    "    df = pd.DataFrame.from_records(records, columns=['dataset', 'model', 'metric', 'value'])\n",
    "    df = pd.pivot_table(df, index='model', columns=['dataset', 'metric'], values='value', aggfunc='first')\n",
    "#     with open('../../data/table.tex', 'w') as fout:\n",
    "#         fout.write(df.to_latex())\n",
    "    \n",
    "    df = df.style.apply(highlight_max)\n",
    "    display(Markdown('# Test metrics summary'))\n",
    "    display(df)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "models = {\n",
    "    'datasets': ['cora', 'citeseer', 'pubmed'],\n",
    "    'bl': {\n",
    "        'base_path': '../../data/metrics/bl/${DATASET}/${NAME}/${VARIANT}.pkl',\n",
    "        'models': [\n",
    "            {'name': 'simple', 'variants': ['full',]},\n",
    "            {'name': 'dw/nf', 'variants': ['full',]},\n",
    "            {'name': 'dw/nfef', 'variants': ['full',]},\n",
    "            {'name': 'n2v/nf', 'variants': ['full',]},\n",
    "            {'name': 'n2v/nfef', 'variants': ['full',]},\n",
    "            {'name': 'sdne/nf', 'variants': ['full',]},\n",
    "            {'name': 'sdne/nfef', 'variants': ['full',]},\n",
    "            {'name': 'struc2vec/nf', 'variants': ['full',]},\n",
    "            {'name': 'struc2vec/nfef', 'variants': ['full',]},\n",
    "            \n",
    "            {'name': 'graphsage/nf', 'variants': ['full',]},\n",
    "            {'name': 'graphsage/nfef', 'variants': ['full',]},\n",
    "        ]\n",
    "    },\n",
    "    'mlp2': {\n",
    "        'base_path': '../../data/metrics/mlp/${DATASET}/${NAME}/MLP2.pkl',\n",
    "        'models': ['dw', 'graphsage', 'n2v', 'sdne', 'struc2vec'],\n",
    "    },\n",
    "    'mlp3': {\n",
    "        'base_path': '../../data/metrics/mlp/${DATASET}/${NAME}/MLP3.pkl',\n",
    "        'models': ['dw', 'graphsage', 'n2v', 'sdne', 'struc2vec'],\n",
    "    },\n",
    "    'line2vec': {\n",
    "        'base_path': '../../data/metrics/bl/${DATASET}/line2vec.pkl',\n",
    "    }, \n",
    "    'ae': {\n",
    "        'base_path': '../../data/metrics/ae/${DATASET}/${NAME}.pkl',\n",
    "        'models': [\n",
    "            'AttrE2vec_Avg',\n",
    "            'AttrE2vec_Exp',\n",
    "            'AttrE2vec_ConcatGRU',\n",
    "            'AttrE2vec_GRU',\n",
    "        ],\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "all_metrics = load_all_metrics(models)\n",
    "\n",
    "print_article_table(all_metrics)\n",
    "\n",
    "summarize_supports(all_metrics)\n",
    "print_auc_table(all_metrics)\n",
    "for metric in ('precision', 'recall', 'f1-score'):\n",
    "    print_metric_table(all_metrics, metric)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
